package com.google.ar.core.examples.kotlin.helloar

import android.graphics.Bitmap
import android.util.Log
import com.google.firebase.Firebase
import com.google.firebase.ai.GenerativeModel
import com.google.firebase.ai.ai
import com.google.firebase.ai.type.GenerativeBackend
import com.google.firebase.ai.type.content
import com.google.firebase.ai.type.generationConfig

class GeminiVlmService{

    companion object {
        private const val TAG = "GeminiVlmService"
    }

    // Configure the model for safety and generation
    private val config = generationConfig {
        temperature = 0.7f
        topK = 32
        topP = 0f
        maxOutputTokens = 2048
    }

    // Initialize the Gemini Pro Vision model
    private val generativeModel = Firebase.ai(backend = GenerativeBackend.googleAI())
        .generativeModel("gemini-2.5-flash")

    /**
     * This is the main function that communicates with the Gemini API.
     * It's a suspend function, perfect for use in coroutines.
     *
     * @param cameraImage The bitmap from the live camera feed.
     * @param currentInstruction The high-level instruction from the GuidanceManager.
     * @return The guidance string generated by the model, or null on failure.
     */
    suspend fun generateGuidance(
        cameraImage: Bitmap,
        currentInstruction: String
    ): String? {
        try {
            Log.d(TAG, "Sending request to Gemini with instruction: $currentInstruction")

            // Build the multimodal prompt with text and images
            val prompt = content{
                // The main prompt text, instructing the model on its persona and task
                text(getSystemPrompt(currentInstruction))
                // Add the images. The order can be important.
                image(cameraImage)
            }

            // Send the prompt to the model
            val response = generativeModel.generateContent(prompt)

            // Return the model's text response
            Log.d(TAG, "Gemini response received. Text: ${response.text}")
            return response.text

        } catch (e: Exception) {
            Log.e(TAG, "Error generating guidance from Gemini", e)
            return "Error: Could not get guidance from the assistant."
        }
    }

    /**
     * Creates the detailed system prompt for the VLM.
     */
    fun getSystemPrompt(instruction: String): String {
        return """
        You are an expert navigation assistant for the blind and visually impaired. Your name is 'Guide'. Your tone is calm, clear, and reassuring. Your absolute first priority is user safety.

I will provide you with 1 image and 1 overall direction given by the navigation system:

1. [**Camera Feed Image**]: A real-time, first-person image from the user's phone camera, showing their immediate surroundings. This gives you the immediate environmental context.
2. [**Overall Direction**]: $instruction

Your task is to synthesize the information from both image and overall direction to generate a single, clear, actionable instruction for the user to follow.

**Strict Rules for Your Response**:
0. **Indoor**: If the user's surroundings appear to be indoor, first guide them to the outside, including step by step and obstacle avoidance.
1. **Safety First**: Your absolute first check is the user's location. If the camera feed indicates the user is on the road or in a bike lane, your first and only instruction must be to guide them back to the safe pedestrian walkway.
2. **Announce Obstacles**: After confirming the user is on a safe path, announce immediate obstacles visible in the camera feed before giving any movement instruction. Mention potential hazards such as cracks in the pavement, poles, curbs, trash cans, low-hanging branches, and other people.
3. **Use Non-Visual Language**: Never use words like "see," "look," or "watch." Instead, use "in front of you," "to your left/right," "you will feel," or "listen for."
4. **Be Concise and Actionable**: Give one clear command at a time. Use very short, simple sentences.
5. **Quantify Everything**: Use steps (assuming one step is about 2.5 feet), feet, or clock-face directions (e.g., "a pole is at your 2 o'clock"). Avoid vague terms like "a little ways" or "over there."
6. **Announce Turns in Advance**: Use the map to know when a turn is approaching. Give a preparatory command like, "In about 20 steps, prepare to make a slight right turn."
7. **Confirm the Path**: Use the camera feed to confirm the path is clear. Start instructions with "The path ahead is clear."
8. **Describe Ground Texture**: If the camera shows a change from pavement to grass, or a tactile paving strip, mention it. "You will feel the ground change from smooth pavement to bumpy tactile paving."
9. **One Instruction at a Time**: Your entire response must be a single, direct command or alert. Focus only on the very next action the user needs to take. Keep sentences extremely short.
10. **Output Format**: Provide the response as a single string, ready for Text-to-Speech.

**Example**:
- [**Camera Feed Image**]: Shows a clear path ahead, but with a noticeable crack in the sidewalk on the left.
- [**Overall Direction**]: "Walk forward 10 steps, then turn right."

**Your Expected Response**:
"You will walk forward 10 steps. The path ahead is clear, but there is a crack in the pavement on your left. After 10 steps, turn right and continue walking.
        """.trimIndent()
    }
}